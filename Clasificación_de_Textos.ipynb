{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+KbUesSytlLcrblIoWXLT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ivano05/Clasificaci-n-de-Textos/blob/main/Clasificaci%C3%B3n_de_Textos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [**Clasificación de opiniones de texto a partir de archivos de texto sin procesar.**](https://)"
      ],
      "metadata": {
        "id": "uvtJxSMxRf0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup\n"
      ],
      "metadata": {
        "id": "S1TJ9N-CGWZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "0V7dNJaCGZpF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos los datos de un sentiment analisis de reseñas de películas de IMDB y examinamos la estructura.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q7Z47tZEHA2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GftqaWayHzi5",
        "outputId": "e3a13765-00ec-45a3-be70-feaf3ce968e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  12.4M      0  0:00:06  0:00:06 --:--:-- 16.4M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La carpeta aclImdb contiene una subcarpeta de entrenamiento y prueba:"
      ],
      "metadata": {
        "id": "_UJDulCLIY2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls aclImdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31qtzJ7-IeMV",
        "outputId": "82fd635c-5f3e-43a9-b815-681ad784dd90"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls aclImdb/test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U03yt0rzIimf",
        "outputId": "0f34b8b0-b40c-4008-db0d-6df22c64e279"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls aclImdb/train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k9uVnYnIyA9",
        "outputId": "62e3ef83-855e-4f62-8801-5242727ee0ce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las carpetas aclImdb/train/pos y aclImdb/train/neg contienen archivos de texto, cada uno de los cuales representa una revisión (ya sea positiva o negativa):"
      ],
      "metadata": {
        "id": "5_hPVhAMI77G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/pos/6248_7.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuj-9lQzI830",
        "outputId": "fa8f7674-bda5-4e94-9f7b-4ebbb944dad9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solo nos interesan las subcarpetas pos y neg, así que eliminemos el resto:"
      ],
      "metadata": {
        "id": "MCXAnyrbJItZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "U4v7Sll9JJbW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puede usar la utilidad tf.keras.preprocessing.text_dataset_from_directory para generar un objeto tf.data.Dataset etiquetado a partir de un conjunto de archivos de texto en el disco archivados en carpetas específicas de clase.\n",
        "\n",
        "Usémoslo para generar los conjuntos de datos de entrenamiento, validación y prueba. Los conjuntos de datos de validación y entrenamiento se generan a partir de dos subconjuntos del directorio de trenes, con el 20 % de las muestras yendo al conjunto de datos de validación y el 80 % al conjunto de datos de entrenamiento.\n",
        "\n",
        "Tener un conjunto de datos de validación además del conjunto de datos de prueba es útil para ajustar hiperparámetros, como la arquitectura del modelo, para los cuales no se debe usar el conjunto de datos de prueba.\n",
        "\n",
        "Sin embargo, antes de poner el modelo en el mundo real, se debe volver a entrenar utilizando todos los datos de entrenamiento disponibles (sin crear un conjunto de datos de validación), para maximizar su rendimiento.\n",
        "\n",
        "Al usar los argumentos de subconjunto y división_de_validación, asegúrese de especificar una semilla aleatoria o pasar shuffle=False, para que las divisiones de validación y entrenamiento que obtenga no se superpongan."
      ],
      "metadata": {
        "id": "Tcp0pwEMJji4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MruHDXwJmCc",
        "outputId": "cbc231ce-1919-4719-904b-ada6e3125ef5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Number of batches in raw_train_ds: 625\n",
            "Number of batches in raw_val_ds: 157\n",
            "Number of batches in raw_test_ds: 782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos una vista previa de algunas muestras:\n"
      ],
      "metadata": {
        "id": "2ZdbjFUEMQ_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Es importante echar un vistazo a sus datos sin procesar para garantizar su normalización.\n",
        "# y la tokenización funcionará como se esperaba. Podemos hacer eso tomando algunos\n",
        "# ejemplos del conjunto de entrenamiento y mirarlos.\n",
        "# Este es uno de los lugares donde brilla la ejecución ansiosa:\n",
        "# simplemente podemos evaluar estos tensores usando .numpy()\n",
        "# en lugar de tener que evaluarlos en un contexto de Sesión/Gráfico.\n",
        "\n",
        "# It's important to take a look at your raw data to ensure your normalization\n",
        "# and tokenization will work as expected. We can do that by taking a few\n",
        "# examples from the training set and looking at them.\n",
        "# This is one of the places where eager execution shines:\n",
        "# we can just evaluate these tensors using .numpy()\n",
        "# instead of needing to evaluate them in a Session/Graph context.\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkI8T_fnMSIZ",
        "outputId": "f82ed81a-ebcb-4694-db44-045a9873a75d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'I am very disappointed with \"K-911.\" The original \"good\" quality of \"K-9\" doesn\\'t exist any more. This is more like a sitcom! Some of casts from original movie returned and got some of my memory back. The captain of Dooley now loves to hit him like a scene from old comedy show. That was crazy. What\\'s the deal with the change of Police? It seems like they are now LAPD! Not San Diego PD. It is a completely different movie from \"'\n",
            "0\n",
            "b\"Giallo fans, seek out this rare film. It is well written, and full of all sorts of the usual low lifes that populate these films. I don't want to give anything away, so I wont even say anything about the plot. The whole movie creates a very bizarre atmosphere, and you don't know what to expect or who to suspect. Recommended! The only place I've seen to get this film in english is from European Trash Cinema, for $15.\"\n",
            "1\n",
            "b\"Terry Gilliam's and David Peoples' teamed up to create one of the most intelligent and creative science fiction movies of the '90's. People's proved a screenplay with bizarre twists and fantastic ideas about the nature of time \\xc2\\x97 I especially love the idea one can't change the past; it's a nice counterpoint to so many time-travelling movies which say otherwise \\xc2\\x97 biological holocausts and the thin line between sanity and madness. Gilliam visualized his ideas with unique quirkiness, perfection and originality.<br /><br />The story itself is engaging: one man, James Cole (played by Bruce Willis in a heart-warming performance) travels several decades to the past to retrieve information about a virus that's wiped out mankind and left only a few survivors alive living underground: with the information he'll collect, scientists hope to find a cure so everyone in the future can return to the surface. But because their time-travelling technology isn't perfect, he ends up being sent towards different other pasts and complicating things. And from that a brilliant science fiction thriller with shades of film noir ensues as the multiple pieces of a huge jigsaw start fitting together to form a bizarre narrative involving animal right activists, end of the millennium paranoia, biological weapons, the perception of reality, and the definition of sanity. With such a complex movie, it was easy for Gilliam and Peoples to create a mess, but instead Twelve Monkeys is a thought-provoking narrative which will please those who like to be challenged and have patience to appreciate some crazy ideas.<br /><br />I watched this movie once around 10 years ago. It marked me a lot: I remember still thinking about many days after-wards; for my young mind this seemed quite mind-blowing and it was one of the first movies to make me appreciate cinema as something serious and important. I've re-watched this movie a few days ago on DVD and it's better than I remembered it. Brad Pitt still steals all the scenes he's in, playing Jeffrey Goines \\xc2\\x97 almost a prelude to his Tyler Durden character in Fight Club \\xc2\\x97 a rich kid with some anarchist/non-conformist ideas who's also crazy and, according to Cole, perhaps responsible for the virus. The scenes between Jeffrey and Cole in the madhouse are the best in the movie, Pitt's eyes, voice and quirky mannerisms convince you he's really a crazy guy locked in a warped logic only he understands. Pitt's Oscar nomination was well deserved! Surprising was also Bruce Willis' performance: his I didn't remember very well, but it's beautiful and full of sensibility; he plays a man who spent almost all his life underground, and when he comes to the past you'll share his childish fascination with something as simple as breathing the fresh air of the morning or watching the sun go up. Cole is a rather ambiguous character, Peoples' tried to imbue some darkness in him, and he does other disturbing things to other people and to himself: the scene where he removes his own teeth reveals how far his dementia has gone unchecked. Ironically Cole didn't start as a crazy character, but when he starts warning everyone about the end of the world, he's considered mad and convinced it's all in his mind, until he arrives at a point when he can't distinguish past from future, reality from fiction. Willis spends a lot of time looking confused and insecure, and it works perfectly. One of the fun twists in the narrative is when Cole's shrink, Dr. Kathryn Railly, finds undeniable proof he's really from the future and now has to convince him again of his mission to save the world. The screenplay is full with weird twists like this and it keeps the movie in a fast pace. Their relationship is also well-handed, although perhaps a bit compressed for time's sake. But I enjoyed watching Cole and Railly falling in love and trying to escape the authority of the future to live a peaceful life in the past. But then things end in a tragic/bittersweet climax at an airport, wrapping all the pieces together, which will blow many minds away.<br /><br />There are two great endings in this movie, a twist in the sense of Se7en or Fight Club, and a more intimate ending where Railly is crouching next to Cole who's just been shot and looking around for a younger James Cole who's witnessing his future self die; the two share a brief look, and she smiles at him. The twist is brilliant, but I prefer this ending for emotional impact. Madeleine Stowe is very good playing Dr. Railly, she drew many different emotions from me in her performance. The movie is filled with a sense of fatalism with the idea the past can't be changed: this movie shows that in a terrifying way. It reminds me of Chinatown in that sense, the way Jake Gittes messes everything up the more he tries to help. Railly's character shares that fatalism, the more she tries to help Cole \\xc2\\x97 first dealing with his 'madness' then helping him in his mission \\xc2\\x97 the more they're sucked into tragedy.<br /><br />The twist ends with a hopeful note, though, with the feeling Cole's mission hasn't been in vain. Twelve Monkeys is a great movie to watch if one wants to be entertained; it's not supposed to be art, although it's more artists than many artistic movies. It's an unpretentious movie where all elements, from music to editing to costume design, etc., came together beautifully to produce a modern cinema masterpiece.\"\n",
            "1\n",
            "b\"We expected something great when we went to see this bomb. It is basically a Broadway play put on film. The music is plain terrible. There isn't one memorable song in the movie -- heard any hits from this movie? You won't because there aren't any. Some of the musical numbers go on so long that I got up to go to the restroom and get some pop corn and it was still going when I got back! If they were good songs well -- but they suck. The pace is slow, terrible character development. The lead was praised for her singing but sounded like she screamed every song -- it was almost impossible to stand. This movie has NOTHING to offer anyone but die-hard Broadway enthusiasts. This is without a doubt the most over rated movie I've seen in my entire life. A complete waist of time and money. There is nothing memorable about this movie except Danny Glover -- who wasn't on screen enough and whose character wasn't developed enough. Rent the video and you'll agree -- this movie was an expensive, over produced, polished dog do.\"\n",
            "0\n",
            "b'I understand this movie was made on a very low budget but that is no excuse for the monstrosity that is Grendel. Deathstalker, The Throne of Fire, Barbarian Queen, Conquest, the Invincible Barbarian were all done on shoestring budgets and poor special effects yet they still managed to create cult classics by adding some scantily clad women warriors and a good sense of humor. The primitive costumes, dark castles and beautiful Bulgarian landscape gave Grendel the potential to be a very good low budget sword and sorcery film, but the makers completely ruined this opportunity by using extremely poor CGI effects and colorless characters. Compare this film to Beowulf (1999). It may not be Citizen Kane but it is a good example of how an entertaining low budget sci-fi/ adventure movie can be made by using credible special effects and appealing characters.'\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparacion de datos\n"
      ],
      "metadata": {
        "id": "xuvFRuy8PWNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En particular, eliminamos las etiquetas <br />.\n",
        "\n"
      ],
      "metadata": {
        "id": "9SzSGH7HPc9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Después de observar nuestros datos anteriores, vemos que el texto sin procesar contiene una ruptura HTML\n",
        "# etiquetas de la forma '<br />'. Estas etiquetas no se eliminarán de forma predeterminada\n",
        "# estandarizador (que no elimina HTML). Debido a esto, tendremos que\n",
        "# crear una función de estandarización personalizada.\n",
        "\n",
        "# Having looked at our data above, we see that the raw text contains HTML break\n",
        "# tags of the form '<br />'. These tags will not be removed by the default\n",
        "# standardizer (which doesn't strip HTML). Because of this, we will need to\n",
        "# create a custom standardization function.\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Constantes del modelo.\n",
        "# Model constants.\n",
        "\n",
        "max_features = 20000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "\n",
        "# Ahora que tenemos nuestra estandarización personalizada, podemos instanciar nuestro texto\n",
        "# capa de vectorización. Estamos usando esta capa para normalizar, dividir y mapear\n",
        "# cadenas a enteros, por lo que establecemos nuestro 'output_mode' en 'int'.\n",
        "# Tenga en cuenta que estamos usando la función de división predeterminada,\n",
        "# y la estandarización personalizada definida anteriormente.\n",
        "# También establecemos una longitud de secuencia máxima explícita, ya que las CNN más adelante en nuestro\n",
        "# el modelo no admitirá secuencias irregulares.\n",
        "\n",
        "\n",
        "# Now that we have our custom standardization, we can instantiate our text\n",
        "# vectorization layer. We are using this layer to normalize, split, and map\n",
        "# strings to integers, so we set our 'output_mode' to 'int'.\n",
        "# Note that we're using the default split function,\n",
        "# and the custom standardization defined above.\n",
        "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
        "# model won't support ragged sequences.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# Ahora que se ha creado la capa de vocabulario, llame a `adapt` en un conjunto \n",
        "# de datos de solo texto para crear el vocabulario. No tiene que realizar lotes, \n",
        "# pero para conjuntos de datos muy grandes, esto significa que no está guardando \n",
        "# copias de repuesto del conjunto de datos en la memoria.\n",
        "\n",
        "\n",
        "# Now that the vocab layer has been created, call `adapt` on a text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
        "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
        "\n",
        "# Hagamos un conjunto de datos de solo texto (sin etiquetas):\n",
        "# Let's make a text-only dataset (no labels):\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "\n",
        "# Llamemos a `adaptar`:\n",
        "# Let's call `adapt`:\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "Kt1bWpmDPnRU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dos opciones para vectorizar los datos Hay 2 formas en que podemos usar nuestra capa de vectorización de texto: Opción 1: Hágalo parte del modelo, para obtener un modelo que procese cadenas en bruto, como este:"
      ],
      "metadata": {
        "id": "9NeCQjI4SjeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "text_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
        "x = vectorize_layer(text_input)\n",
        "x = layers.Embedding(max_features + 1, embedding_dim)(x)\n",
        "...\n",
        "\n",
        "Opción 2: aplíquelo al conjunto de datos de texto para obtener un conjunto de datos de índices de palabras, luego introdúzcalo en un modelo que espera secuencias enteras como entradas.\n",
        "\n",
        "Una diferencia importante entre los dos es que la opción 2 le permite realizar procesamiento de CPU asíncrono y almacenamiento en búfer de sus datos cuando entrena en GPU. Entonces, si está entrenando el modelo en GPU, probablemente desee optar por esta opción para obtener el mejor rendimiento. Esto es lo que haremos a continuación.\n",
        "\n",
        "Si tuviéramos que exportar nuestro modelo a producción, enviaríamos un modelo que acepta cadenas sin formato como entrada, como en el fragmento de código de la opción 1 anterior. Esto se puede hacer después del entrenamiento. Hacemos esto en la última sección."
      ],
      "metadata": {
        "id": "f96MnvkfSsX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "# Vectorizar los datos.\n",
        "# Vectorize the data.\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "\n",
        "# Realice una captura previa / \n",
        "# almacenamiento en búfer asincrónico de los datos para obtener el mejor rendimiento en la GPU.\n",
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ],
      "metadata": {
        "id": "soBoGNE7S6mo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construir un modelo\n",
        "\n",
        "Elegimos una convnet 1D simple que comienza con una capa de incrustación."
      ],
      "metadata": {
        "id": "OYI8HbsFTGfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "# Una entrada de número entero para los índices de vocabulario.\n",
        "# A integer input for vocab indices.\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "\n",
        "# A continuación, agregamos una capa para mapear esos índices de vocabulario \n",
        "# en un espacio de dimensionalidad 'embedding_dim'.\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Conv1D + agrupación máxima global\n",
        "# Conv1D + global max pooling\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "\n",
        "# Agregamos una capa oculta de vainilla:\n",
        "# We add a vanilla hidden layer:\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "\n",
        "# Proyectamos en una capa de salida de una sola unidad y la aplastamos con un sigmoide:\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "\n",
        "# Compile el modelo con pérdida de entropía cruzada binaria y un optimizador Adam.\n",
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "fHutWoARTTft"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenar el modelo"
      ],
      "metadata": {
        "id": "i89VZSoZTeaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "\n",
        "\n",
        "# Ajuste el modelo usando los conjuntos de datos de entrenamiento y prueba.\n",
        "# Fit the model using the train and test datasets.\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1FK29RMThKi",
        "outputId": "94a3c4ee-abe7-4a93-c656-4140bd7fda66"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "625/625 [==============================] - 122s 193ms/step - loss: 0.4857 - accuracy: 0.7304 - val_loss: 0.3051 - val_accuracy: 0.8762\n",
            "Epoch 2/3\n",
            "625/625 [==============================] - 116s 186ms/step - loss: 0.2185 - accuracy: 0.9152 - val_loss: 0.3199 - val_accuracy: 0.8744\n",
            "Epoch 3/3\n",
            "625/625 [==============================] - 115s 184ms/step - loss: 0.1178 - accuracy: 0.9563 - val_loss: 0.3790 - val_accuracy: 0.8728\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdbeda26a50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluar el modelo en el set de prueba\n"
      ],
      "metadata": {
        "id": "MOtg5iY7VQNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcStKNt2VT0c",
        "outputId": "8d597f6c-8d3d-44f6-fdd7-d4eb72851459"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 34s 44ms/step - loss: 0.3894 - accuracy: 0.8632\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3893643319606781, 0.8632400035858154]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacer un modelo de punta a punta\n",
        "\n",
        "\n",
        "Si desea obtener un modelo capaz de procesar cadenas en bruto, simplemente puede\n",
        "crear un nuevo modelo (usando los pesos que acabamos de entrenar):\n"
      ],
      "metadata": {
        "id": "ZqVOuMHaVdQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Una entrada de cadena\n",
        "# A string input\n",
        "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
        "\n",
        "# Convierte cadenas en índices de vocabulario\n",
        "# Turn strings into vocab indices\n",
        "indices = vectorize_layer(inputs)\n",
        "\n",
        "# Convierta los índices de vocabulario en predicciones\n",
        "# Turn vocab indices into predictions\n",
        "outputs = model(indices)\n",
        "\n",
        "# Nuestro modelo de extremo a extremo\n",
        "# Our end to end model\n",
        "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
        "end_to_end_model.compile(\n",
        "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "# Pruébelo con `raw_test_ds`, que produce cadenas sin procesar\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "end_to_end_model.evaluate(raw_test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oHa54g-VniA",
        "outputId": "9d7e9bf9-4d36-4e7d-d5c8-71e1dfe7885b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 33s 42ms/step - loss: 0.3894 - accuracy: 0.8632\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3893642723560333, 0.8632400035858154]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}